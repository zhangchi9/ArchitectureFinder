{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/zhang.chi9/anaconda3/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhang.chi9/anaconda3/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhang.chi9/anaconda3/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhang.chi9/anaconda3/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhang.chi9/anaconda3/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhang.chi9/anaconda3/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/scratch/zhang.chi9/architectural-styles-dataset/'\n",
    "\n",
    "# def split_files(source, destination):\n",
    "#     files = os.listdir(source)\n",
    "#     for f in files:\n",
    "#         if np.random.rand(1) < 0.2:\n",
    "#             shutil.move(source + '/'+ f, destination + '/'+ f)\n",
    "\n",
    "# for folder_name in os.listdir(data_dir):\n",
    "#     source = data_dir + folder_name\n",
    "#     destination = '/scratch/zhang.chi9/test/' + folder_name\n",
    "#     if not os.path.isdir(destination):\n",
    "#         os.makedirs(destination)\n",
    "#         split_files(source,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchify(jc):\n",
    "\n",
    "    jc = np.uint8(jc)\n",
    "    \n",
    "    scale_percent = 1\n",
    "\n",
    "    width = int(jc.shape[1]*scale_percent)\n",
    "    height = int(jc.shape[0]*scale_percent)\n",
    "\n",
    "    dim = (width,height)\n",
    "    resized = cv2.resize(jc,dim,interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    kernel_sharpening = np.array([[-1,-1,-1], \n",
    "                                  [-1, 9,-1],\n",
    "                                  [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(resized,-1,kernel_sharpening)\n",
    "\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(sharpened , cv2.COLOR_BGR2GRAY)\n",
    "    inv = 255-gray\n",
    "    gauss = cv2.GaussianBlur(inv,ksize=(15,15),sigmaX=0,sigmaY=0)\n",
    "\n",
    "    def dodgeV2(image,mask):\n",
    "        return cv2.divide(image,255-mask,scale=256)\n",
    "\n",
    "    pencil_jc = 255 - dodgeV2(gray,gauss)\n",
    "    img = np.zeros(jc.shape)\n",
    "    img[:,:,0] = pencil_jc\n",
    "    img[:,:,1] = pencil_jc\n",
    "    img[:,:,2] = pencil_jc\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = preprocessing.image.ImageDataGenerator(rescale=1/255.0,horizontal_flip=True,preprocessing_function=sketchify) #\n",
    "testgen = preprocessing.image.ImageDataGenerator(rescale=1/255.0,horizontal_flip=True,preprocessing_function=sketchify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12308 images belonging to 25 classes.\n",
      "Found 3124 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "train = '/scratch/zhang.chi9/train/'\n",
    "test = '/scratch/zhang.chi9/test/'\n",
    "train_generator = traingen.flow_from_directory(train)\n",
    "test_generator = testgen.flow_from_directory(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    include_top=False) \n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(None, None, 3))\n",
    "x = base_model(inputs)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.8)(x)\n",
    "outputs = keras.layers.Dense(25,activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "385/385 [==============================] - 494s 1s/step - loss: 2.8863 - categorical_accuracy: 0.1663 - val_loss: 3.1398 - val_categorical_accuracy: 0.1108\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.11076, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 2/10\n",
      "385/385 [==============================] - 192s 499ms/step - loss: 2.4557 - categorical_accuracy: 0.2792 - val_loss: 3.1159 - val_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.11076 to 0.13924, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 3/10\n",
      "385/385 [==============================] - 167s 434ms/step - loss: 2.3203 - categorical_accuracy: 0.3155 - val_loss: 3.3081 - val_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.13924 to 0.14309, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 4/10\n",
      "385/385 [==============================] - 153s 398ms/step - loss: 2.2511 - categorical_accuracy: 0.3341 - val_loss: 3.3909 - val_categorical_accuracy: 0.1511\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.14309 to 0.15109, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 5/10\n",
      "385/385 [==============================] - 148s 384ms/step - loss: 2.2135 - categorical_accuracy: 0.3441 - val_loss: 3.5340 - val_categorical_accuracy: 0.1428\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.15109\n",
      "Epoch 6/10\n",
      "385/385 [==============================] - 152s 396ms/step - loss: 2.1957 - categorical_accuracy: 0.3449 - val_loss: 3.4572 - val_categorical_accuracy: 0.1463\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.15109\n",
      "Epoch 7/10\n",
      "385/385 [==============================] - 148s 385ms/step - loss: 2.1775 - categorical_accuracy: 0.3550 - val_loss: 3.6231 - val_categorical_accuracy: 0.1456\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.15109\n",
      "Epoch 8/10\n",
      "385/385 [==============================] - 150s 389ms/step - loss: 2.1803 - categorical_accuracy: 0.3565 - val_loss: 3.6302 - val_categorical_accuracy: 0.1415\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.15109\n",
      "Epoch 9/10\n",
      "385/385 [==============================] - 148s 384ms/step - loss: 2.1603 - categorical_accuracy: 0.3595 - val_loss: 3.4454 - val_categorical_accuracy: 0.1629\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.15109 to 0.16293, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 10/10\n",
      "385/385 [==============================] - 145s 376ms/step - loss: 2.1693 - categorical_accuracy: 0.3566 - val_loss: 3.4950 - val_categorical_accuracy: 0.1530\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.16293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aba1f6de438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=categorical_crossentropy,\n",
    "              metrics=[categorical_accuracy])\n",
    "\n",
    "weights_path = '/scratch/zhang.chi9/arch_weights/'\n",
    "best_acc = ModelCheckpoint(f'{weights_path}best_acc_model.h5', monitor='val_categorical_accuracy',save_best_only=True, mode='max',verbose = 1)\n",
    "model.fit_generator(train_generator, validation_data = test_generator, epochs = 10,callbacks=[best_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
    "              loss=categorical_crossentropy,\n",
    "              metrics=[categorical_accuracy])\n",
    "weights_path = '/scratch/zhang.chi9/arch_weights/'\n",
    "best_acc = ModelCheckpoint(f'{weights_path}best_acc_model.h5', monitor='val_categorical_accuracy',save_best_only=True, mode='max',verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "385/385 [==============================] - 329s 856ms/step - loss: 0.2446 - categorical_accuracy: 0.9292 - val_loss: 0.7559 - val_categorical_accuracy: 0.8095\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from 0.80730 to 0.80954, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 2/20\n",
      "385/385 [==============================] - 326s 848ms/step - loss: 0.2142 - categorical_accuracy: 0.9405 - val_loss: 0.7504 - val_categorical_accuracy: 0.8140\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.80954 to 0.81402, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 3/20\n",
      "385/385 [==============================] - 326s 847ms/step - loss: 0.1950 - categorical_accuracy: 0.9446 - val_loss: 0.7540 - val_categorical_accuracy: 0.8166\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.81402 to 0.81658, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 4/20\n",
      "385/385 [==============================] - 327s 851ms/step - loss: 0.1713 - categorical_accuracy: 0.9532 - val_loss: 0.7542 - val_categorical_accuracy: 0.8249\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.81658 to 0.82490, saving model to /scratch/zhang.chi9/arch_weights/best_acc_model.h5\n",
      "Epoch 5/20\n",
      "385/385 [==============================] - 328s 853ms/step - loss: 0.1485 - categorical_accuracy: 0.9598 - val_loss: 0.7500 - val_categorical_accuracy: 0.8220\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.82490\n",
      "Epoch 6/20\n",
      "385/385 [==============================] - 328s 853ms/step - loss: 0.1313 - categorical_accuracy: 0.9676 - val_loss: 0.7304 - val_categorical_accuracy: 0.8220\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.82490\n",
      "Epoch 7/20\n",
      "384/385 [============================>.] - ETA: 0s - loss: 0.1196 - categorical_accuracy: 0.9679"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, validation_data = test_generator, epochs = 20,callbacks=[best_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = '/scratch/zhang.chi9/arch_weights/'\n",
    "model = load_model(f'{weights_path}best_acc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "xception (Model)             (None, None, None, 2048)  20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                51225     \n",
      "=================================================================\n",
      "Total params: 20,912,705\n",
      "Trainable params: 20,858,177\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Model(inputs = model.input, outputs = model.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.batch_size = 12308\n",
    "x_train, y_train = next(train_generator)\n",
    "test_generator.batch_size = 3124\n",
    "x_test, y_test = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train = embedding_model.predict(x_train)\n",
    "embedding_test = embedding_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.concatenate([embedding_train,embedding_test],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15432, 2048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-05d64dde3ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfgpu/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfgpu/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfgpu/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m# optionally save augmented images to disk for debugging purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfgpu/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \"\"\"\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3ca78ea594f2>\u001b[0m in \u001b[0;36msketchify\u001b[0;34m(jc)\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   [-1,-1,-1]])\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msharpened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_sharpening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in test_testerator:\n",
    "    idx = (test.batch_index - 1) * test.batch_size\n",
    "    print(test.filenames[idx : idx + test.batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./all_embedding.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('building.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img/255;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = embedding_model.predict(img[np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = []\n",
    "for i in range(len(embedding)):\n",
    "    vector1 = embedding[i,:]\n",
    "    cosine_similarity.append(1 - spatial.distance.cosine(vector1, xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(cosine_similarity),np.max(cosine_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11310 - 3124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[1171,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(y_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([principalComponents,y_label[:,np.newaxis]],axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data,columns=['PC1','PC2','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pca.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "for class_ in range(7):\n",
    "    plt.scatter(df.loc[df.label==class_,'PC1'],df.loc[df.label==class_,'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f'The accuracy score is {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "class_names = ['Achaemenid architecture', 'American Foursquare architecture', 'American craftsman style', 'Ancient Egyptian architecture', 'Art Deco architecture', 'Art Nouveau architecture', 'Baroque architecture', 'Bauhaus architecture', 'Beaux-Arts architecture', 'Byzantine architecture', 'Chicago school architecture', 'Colonial architecture', 'Deconstructivism', 'Edwardian architecture', 'Georgian architecture', 'Gothic architecture', 'Greek Revival architecture', 'International style', 'Novelty architecture', 'Palladian architecture', 'Postmodern architecture', 'Queen Anne architecture', 'Romanesque architecture', 'Russian Revival architecture', 'Tudor Revival architecture']\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    fig,ax = plt.subplots(figsize=(16,16))\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.grid(False)\n",
    "    plt.title(title)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(16,16))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = embedding_model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.classes[0:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4,figsize = (16,16))\n",
    "for i in range(0,16):\n",
    "    x, y = int(i/4), i%4\n",
    "    ax[x][y].imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4,figsize = (16,16))\n",
    "for i in range(0,16):\n",
    "    x, y = int(i/4), i%4\n",
    "    ax[x][y].imshow(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
