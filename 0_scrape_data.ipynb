{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script scrape images from archdaily.com and save them into AWS S3 bucket. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import psycopg2\n",
    "import config as creds\n",
    "import random\n",
    "import string\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from webapp.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_alphanumeric_string(length):\n",
    "    \"\"\"\n",
    "    Generate a random alphanumeric string for naming each image scraped\n",
    "    input: \n",
    "        length: int\n",
    "            the length of the alphanumeric string\n",
    "    output:\n",
    "        result_str: string\n",
    "            the generated random string\n",
    "    \"\"\"\n",
    "    letters_and_digits = string.ascii_letters + string.digits\n",
    "    result_str = ''.join((random.choice(letters_and_digits) for i in range(length)))\n",
    "    return result_str\n",
    "\n",
    "def is_true_image(url):\n",
    "    \"\"\"\n",
    "    Return if the image in the url is a image or not\n",
    "    input:\n",
    "        url: string\n",
    "            the url address of the image\n",
    "    output: \n",
    "        boolean\n",
    "            if the image in the url is a ture image or not\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = np.array(img)\n",
    "    if len(img.shape) != 3:\n",
    "        return False\n",
    "    if img.shape[2] != 3:\n",
    "        return False\n",
    "    if np.sum(img == 255)/img.shape[0]/img.shape[1]/img.shape[2] > 0.1:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project:\n",
    "    \"\"\"\n",
    "    This class is create for each project.\n",
    "    \n",
    "    Properties:\n",
    "        project_name string\n",
    "        project_url: string\n",
    "        first_image_url: string\n",
    "    \n",
    "    Methods:\n",
    "        get_first_image_url\n",
    "        get_other_info\n",
    "        update_projects_table\n",
    "        update_images_table\n",
    "        down_load_all_images\n",
    "    \"\"\"\n",
    "    def __init__(self,project_name, project_url):\n",
    "        self.project_name = project_name\n",
    "        self.project_url = project_url\n",
    "        driver.get(self.project_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        self.get_first_image_url(soup)\n",
    "        self.get_other_info(soup)\n",
    "        \n",
    "    def get_first_image_url(self,soup):\n",
    "        \"\"\"\n",
    "        input: project_url\n",
    "        output: all image urls\n",
    "        \"\"\"\n",
    "        x = soup.find('a',{'class':'js-image-size__link'})\n",
    "        image_url = 'https://www.archdaily.com/' + x.attrs['href']\n",
    "        self.first_image_url = image_url\n",
    "        \n",
    "    def get_other_info(self,soup):\n",
    "        \"\"\"\n",
    "        get all other related information, such as categories, location, architects \n",
    "        \"\"\"\n",
    "        if soup.find('div',\"afd-specs__header-category\"):\n",
    "            self.categories = soup.find('div',\"afd-specs__header-category\").text.replace(\"'\", \"\")\n",
    "\n",
    "        if soup.find('div',\"afd-specs__header-location\"):\n",
    "            self.location = soup.find('div',\"afd-specs__header-location\").text.replace(\"'\", \"\")\n",
    "\n",
    "        if soup.find('div',\"afd-specs__architects\"):\n",
    "            self.architects = soup.find('div',\"afd-specs__architects\").text[13:].replace(\"'\", \"\")\n",
    "        \n",
    "    def update_projects_table(self):\n",
    "        \"\"\"\n",
    "        update the project table in AWS RDS \n",
    "        \"\"\"\n",
    "        cursor.execute(f'INSERT INTO public.\"Projects\" (url, categories, architects, location, name) \\\n",
    "        VALUES (\\'{self.project_url}\\', \\'{self.categories}\\', \\'{self.architects}\\', \\'{self.location}\\', \\'{self.project_name}\\');')\n",
    "\n",
    "        conn.commit()\n",
    "        print(f'added {self.project_name} in db!')\n",
    "        \n",
    "    def update_images_table(self, name, url, path):\n",
    "         \"\"\"\n",
    "        update the images table in AWS RDS \n",
    "        \"\"\"\n",
    "        url = url.replace(\"'\", \"\")\n",
    "        cursor.execute(f'INSERT INTO public.\"Images\" (name, url, project_name, path) \\\n",
    "        VALUES (\\'{name}\\', \\'{url}\\', \\'{self.project_name}\\', \\'{path}\\');')\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "    def down_load_all_images(self):\n",
    "        \"\"\"\n",
    "        down load all images contained in the project and save it into AWS s3 bucket\n",
    "        \"\"\"\n",
    "        print(f'downing all images from project {self.project_name}')\n",
    "        driver.get(self.first_image_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        directory = f'{image_data_dir}/{self.project_name}'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        download_urls = json.loads(soup.find('div', {'id':'gallery-items'}).attrs['data-images'])\n",
    "\n",
    "        for i in range(len(download_urls)):\n",
    "            response = requests.get(download_urls[i]['url_slideshow'])\n",
    "            \n",
    "            if is_true_image(download_urls[i]['url_slideshow']):\n",
    "                filename = get_random_alphanumeric_string(10)\n",
    "                file = open(f\"{directory}/{i}_{filename}.png\", \"wb\")\n",
    "\n",
    "                self.update_images_table(f'{i}_{filename}.png',download_urls[i]['url_slideshow'],f\"{directory}/{i}_{filename}.png\")\n",
    "\n",
    "                file.write(response.content)\n",
    "                file.close()\n",
    "                \n",
    "        self.update_projects_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Page:\n",
    "    \"\"\"\n",
    "    This class is create for each page of archdaily.\n",
    "    \n",
    "    Properties:\n",
    "        page_name: string\n",
    "        page_url: string\n",
    "        projects: list[Project]\n",
    "    \n",
    "    Methods:\n",
    "        get_all_projects\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, page_name, page_url):\n",
    "        self.page_name = page_name\n",
    "        self.page_url = page_url\n",
    "        self.projects = []\n",
    "        self.get_all_projects(URL)\n",
    "        \n",
    "    def get_all_projects(self,URL):\n",
    "        \"\"\"\n",
    "        input: a page url\n",
    "        return: list of projects_titles in this page\n",
    "                list of projects_urls in this page\n",
    "        \"\"\"\n",
    "        project_urls = []\n",
    "        project_titles = []\n",
    "        driver.get(URL)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        gridview__content = soup.find_all('a', {'class':'gridview__content'})\n",
    "\n",
    "        for element in gridview__content:\n",
    "            project_url = element.attrs['href']\n",
    "            project_name = element.find('h3',{'class':\"gridview__entry-title\"}).text\n",
    "            project_name = project_name.replace('/','_').replace(\"'\", \"\")\n",
    "            cursor.execute(f'SELECT * FROM public.\"Projects\" where name = \\'{project_name}\\'')\n",
    "            if cursor.rowcount == 0:\n",
    "                project = Project(project_name,project_url)\n",
    "                self.projects.append(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn, cursor = connect()\n",
    "image_data_dir = '/Users/chizhang/AWS-s3/archdaily'\n",
    "for page_num in range(0,600):\n",
    "    # Loop over page number\n",
    "    driver = webdriver.Chrome('../chromedriver')\n",
    "    print(f'------------scraping page {page_num}...------------------')\n",
    "    URL = f'https://www.archdaily.com/search/projects?page={page_num}'\n",
    "\n",
    "    page_readed = False\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            page = Page(f'{page_num}',URL)\n",
    "            page_readed = True\n",
    "        except:\n",
    "            continue\n",
    "        break\n",
    "        \n",
    "    if not page_readed:\n",
    "        driver.quit()\n",
    "        continue\n",
    "\n",
    "    for project in page.projects:\n",
    "        # Loop ove all project \n",
    "        try:\n",
    "            project.down_load_all_images()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    driver.quit()\n",
    "    print(f'done scrape page {page_num}')\n",
    "    \n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
